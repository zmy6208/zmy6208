{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zmy6208/zmy6208/blob/main/Pytorch_Lightning_csimcse_classification_20240629.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 実行環境の設定"
      ],
      "metadata": {
        "id": "mKbqZNE5RncB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfR7GvT1RQKI",
        "outputId": "a888dc65-8d09-42f5-b5fb-88c87852a39c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 29 07:19:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0              52W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Python 3.10.12\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "###\n",
        "# GPUおよびPython環境のチェック\n",
        "#import locale\n",
        "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!nvidia-smi\n",
        "!python --version\n",
        "\n",
        "###\n",
        "# Google Drive のマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# パッケージインポート"
      ],
      "metadata": {
        "id": "1M96SqGESCVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import openpyxl as op\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "#!pip install torch\n",
        "#from torch import torch\n",
        "#from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "!pip install pytorch_lightning --quiet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "#\n",
        "#from sklearn.metrics import adjusted_mutual_info_score\n",
        "#from sklearn.metrics import confusion_matrix, classification_report\n",
        "#from sklearn.metrics.pairwise import paired_cosine_distances\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "#\n",
        "#from sklearn.decomposition import PCA\n",
        "#from sklearn.preprocessing import minmax_scale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw6kCIyER76R",
        "outputId": "52225f42-659a-4a4b-b6d8-31adacd64582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence_transformers-3.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# パラメータ設定"
      ],
      "metadata": {
        "id": "98abLdG3TRfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### cyclone simcseのパス\n",
        "cyclone_model_name_or_path = \"/content/gdrive/MyDrive/cyclone-simcse-chinese-roberta-wwm-ext\"\n",
        "\n",
        "### excelデータファイルパス\n",
        "PATH_SEPARATOR = \"/\"\n",
        "#\n",
        "INPUT_DIR = \"/content/gdrive/MyDrive/csimcse_data/input/Chinese_testdata/\"\n",
        "OUTPUT_DIR = \"/content/gdrive/MyDrive/csimcse_data/output/Chinese_testdata/\"\n",
        "#\n",
        "INPUT_FILE = \"/content/gdrive/MyDrive/csimcse_data/input/Corpus-Review+Corpus-Novel.xlsx\"\n",
        "\n",
        "### 11感情カテゴリ\n",
        "EMOTION_CATEGORIES = ['joy','anger','sadness','fear','shame','like','dislike','excitement','peacefulness','surprise','request']\n",
        "\n",
        "### corpusで実行(True)、またはirisテストで実行(False)\n",
        "CORPUS_MODE = True\n",
        "\n",
        "### DNNパラメータ\n",
        "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
        "#print(AVAIL_GPUS)\n",
        "DROPOUT = 0.5\n",
        "VERBOSE = True\n",
        "BATCH_SIZE = 10\n",
        "EPOCK = 100\n",
        "\n",
        "###\n",
        "MODEL_SAVE_PATH = \"/content/gdrive/MyDrive/csimcse_data/model/\""
      ],
      "metadata": {
        "id": "ju0oUjTpTR_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ファイル入出力ユーティリティ"
      ],
      "metadata": {
        "id": "W30WY-zLTpSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### jsonファイル入出力\n",
        "# jsonファイルの読み込み：必要な列数に書き換える\n",
        "def read_json_file(JSON_DATAFILE):\n",
        "    pd_data = pd.read_json(JSON_DATAFILE)\n",
        "    ndaray_data = pd_data.to_numpy(copy=True).transpose()\n",
        "    list_first_column = list(ndaray_data[0])\n",
        "    list_second_column = list(ndaray_data[1])\n",
        "    #print(list_first_column)\n",
        "    #print(list_second_column)\n",
        "    return list_first_column, list_second_column\n",
        "\n",
        "# 2次元list_dataリストのjsonファイルへの書き出し\n",
        "def write_json_file(JSON_DATAFILE, list_data):\n",
        "    with open(JSON_DATAFILE, \"w\") as f:\n",
        "        json.dump(list_data, f)\n",
        "\n",
        "### excelファイル入出力\n",
        "# excelデータの読み込み\n",
        "def read_excel_file(EXCEL_DATAFILE):\n",
        "    input_book = pd.ExcelFile(EXCEL_DATAFILE)\n",
        "    input_sheet_name = input_book.sheet_names\n",
        "    input_sheet_df = input_book.parse(input_sheet_name[0])\n",
        "    transposed_df = input_sheet_df.transpose()\n",
        "    data = transposed_df.values.tolist()\n",
        "    #print(\"Number of columns:\", len(data))\n",
        "    if len(data) == 13:                              # Corpus for emotional analysis\n",
        "        sentences = data[1]\n",
        "        emotion_data = np.array(data[2:13])\n",
        "    else:\n",
        "        print(\"Excel : not supported format\")\n",
        "        sys.exit()\n",
        "    return len(data), sentences, emotion_data\n",
        "\n",
        "# excelファイルへの書き出し\n",
        "def write_excel_file(EXCEL_DATAFILE, pd_data):\n",
        "    pd_data.to_excel(EXCEL_DATAFILE, index=False, header=False)\n",
        "\n",
        "## textファイルの入出力\n",
        "# textデータの読み込み\n",
        "def read_text_file(TEXT_DATAFILE):\n",
        "    sentences = []\n",
        "    with open(TEXT_DATAFILE, mode='r', encoding='utf-8') as f:\n",
        "        for s in f:\n",
        "            try:\n",
        "                sentences.append(s.rstrip())\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return sentences\n",
        "\n",
        "# textファイルへの書き出し\n",
        "def write_text_file(TEXT_DATAFILE, sentences):\n",
        "    with open(TEXT_DATAFILE, mode='w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(sentences))"
      ],
      "metadata": {
        "id": "PgrO-0oZTpvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chinese SimCSEによる文埋め込みの生成"
      ],
      "metadata": {
        "id": "FPVg1knBTztH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Chinese SimCSEモデル\n",
        "# Cyclone simcseのパス\n",
        "cyclone_model_name_or_path = \"/content/gdrive/MyDrive/cyclone-simcse-chinese-roberta-wwm-ext\"\n",
        "# Chinese SimCSEモデルの生成\n",
        "cyclone_simcse_model = SentenceTransformer(cyclone_model_name_or_path)\n",
        "\n",
        "### 文埋め込みと各感情情報の取得\n",
        "def obtain_sentence_emdeddings_categories():\n",
        "    print(\"Corpusの読込\")\n",
        "    emotion_labels = {}         # {感情カテゴリ, 全文に対する感情を持つ(1)か否か(0)を表すベクトル}\n",
        "    _num, sentences, emotion_data = read_excel_file(INPUT_FILE)\n",
        "    #print(num)\n",
        "    #\n",
        "    print(\"文埋め込みの生成開始\")\n",
        "    sentence_embeddings = cyclone_simcse_model.encode(sentences)\n",
        "    #print(type(sentence_embeddings), sentence_embeddings.shape)\n",
        "    #\n",
        "    print(\"各感情ベクトルの生成\")\n",
        "    for e, label in enumerate(emotion_data):\n",
        "        #print(EMOTION_CATEGORIES[e])\n",
        "        #print(e, len(label))\n",
        "        emotion_labels[EMOTION_CATEGORIES[e]] = label\n",
        "    return sentences, sentence_embeddings, emotion_labels\n",
        "\n",
        "def check_emotional_categories():\n",
        "    sentences, sentence_embeddings, emotion_labels = obtain_sentence_emdeddings_categories()\n",
        "    print(type(sentences), type(sentence_embeddings))\n",
        "    print(\"Total senteces:\", len(sentences))\n",
        "    for k, v in emotion_labels.items():\n",
        "        print(type(v))\n",
        "        print(k, np.count_nonzero(v == 1))\n",
        "\n",
        "#check_emotional_categories()"
      ],
      "metadata": {
        "id": "0w3uk19xT0Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Lightningを用いたDNN"
      ],
      "metadata": {
        "id": "4D7l-JyPUBaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "def split_train_val_test(x, t):\n",
        "    # PyTorch の Tensor 型へ変換\n",
        "    x = torch.tensor(x, dtype=torch.float32)\n",
        "    t = torch.tensor(t, dtype=torch.int64)\n",
        "\n",
        "    # 入力値と目標値をまとめる\n",
        "    dataset = torch.utils.data.TensorDataset(x, t)\n",
        "\n",
        "    # 各データセットのサンプル数を決定\n",
        "    # train : val: test = 60%　: 20% : 20%\n",
        "    n_train = int(len(dataset) * 0.6)\n",
        "    n_val = int(len(dataset) * 0.2)\n",
        "    n_test = len(dataset) - n_train - n_val\n",
        "\n",
        "    # データセットの分割\n",
        "    # ランダムに分割を行うため、シードを固定して再現性を確保\n",
        "    torch.manual_seed(0)\n",
        "    train, val, test = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
        "    n_test = len(dataset) - n_train - n_val\n",
        "    return train, val, test\n",
        "\n",
        "###\n",
        "# Chinese corpusのtrain, val, testデータの取得\n",
        "def obtain_corpus_train_val_test(num):\n",
        "    _sentences, sentence_embeddings, emotion_labels = obtain_sentence_emdeddings_categories()\n",
        "    #print(sentence_embeddings.shape, len(sentence_embeddings[0]), emotion_labels[EMOTION_CATEGORIES[num]].shape)\n",
        "    train, val, test = split_train_val_test(sentence_embeddings, emotion_labels[EMOTION_CATEGORIES[num]])\n",
        "    total_size = len(sentence_embeddings)\n",
        "    in_dim = len(sentence_embeddings[0])\n",
        "    out_dim = len(np.unique(emotion_labels[EMOTION_CATEGORIES[num]]))\n",
        "    #print(total_size,in_dim, out_dim)\n",
        "    return total_size, in_dim, out_dim, train, val, test\n",
        "\n",
        "#obtain_corpus_train_val_test()"
      ],
      "metadata": {
        "id": "7fairOfuUB1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DNNモデルの定義"
      ],
      "metadata": {
        "id": "xoXdyjZtUhZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "def generate_DNN_model(in_dim, out_dim, train, val, test):\n",
        "    # バッチサイズの定義\n",
        "    batch_size = BATCH_SIZE\n",
        "\n",
        "    # Data Loader を用意\n",
        "    # shuffle はデフォルトで False のため、訓練データのみ True に指定\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size)\n",
        "\n",
        "    class Network(pl.LightningModule):\n",
        "\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            input_dim=in_dim\n",
        "            hidden_dim=in_dim\n",
        "            output_dim=out_dim\n",
        "            dropout=DROPOUT\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.batch_size = batch_size\n",
        "            self.fc1 = nn.Linear(in_dim, in_dim)\n",
        "            self.fc2 = nn.Linear(in_dim, in_dim)\n",
        "            self.fc3 = nn.Linear(in_dim, in_dim)\n",
        "            #self.fc4 = nn.Linear(in_dim, in_dim)\n",
        "            #self.fc5 = nn.Linear(in_dim, in_dim)\n",
        "            #self.fc6 = nn.Linear(in_dim, in_dim)\n",
        "            self.fc_out = nn.Linear(in_dim, out_dim)\n",
        "            self.bn = nn.BatchNorm1d(in_dim)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.bn(x)\n",
        "            h = self.fc1(x)\n",
        "            h = F.relu(h)\n",
        "            h = self.dropout(h)\n",
        "            h = self.fc2(h)\n",
        "            h = F.relu(h)\n",
        "            h = self.dropout(h)\n",
        "            h = self.fc3(h)\n",
        "            h = F.relu(h)\n",
        "            h = self.dropout(h)\n",
        "            #h = self.fc4(h)\n",
        "            #h = F.relu(h)\n",
        "            #h = self.dropout(h)\n",
        "            #h = self.fc5(h)\n",
        "            #h = F.relu(h)\n",
        "            #h = self.dropout(h)\n",
        "            #h = self.fc6(h)\n",
        "            #h = F.relu(h)\n",
        "            #h = self.dropout(h)\n",
        "            h = self.fc_out(h)\n",
        "            return h\n",
        "\n",
        "        def training_step(self, batch, batch_idx):\n",
        "            x, t = batch\n",
        "            y = self(x)\n",
        "            loss = F.cross_entropy(y, t)\n",
        "            y_label = torch.argmax(y, dim=1)\n",
        "            acc = torch.sum(t == y_label) * 1.0 / len(t)\n",
        "            self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "            self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
        "            return loss\n",
        "\n",
        "        def validation_step(self, batch, batch_idx):\n",
        "            x, t = batch\n",
        "            y = self(x)\n",
        "            loss = F.cross_entropy(y, t)\n",
        "            y_label = torch.argmax(y, dim=1)\n",
        "            acc = torch.sum(t == y_label) * 1.0 / len(t)\n",
        "            self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
        "            self.log('val_acc', acc, on_step=False, on_epoch=True)\n",
        "            return loss\n",
        "\n",
        "        def test_step(self, batch, batch_idx):\n",
        "            x, t = batch\n",
        "            y = self(x)\n",
        "            loss = F.cross_entropy(y, t)\n",
        "            y_label = torch.argmax(y, dim=1)\n",
        "            acc = torch.sum(t == y_label) * 1.0 / len(t)\n",
        "            self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
        "            self.log('test_acc', acc, on_step=False, on_epoch=True)\n",
        "            return loss\n",
        "\n",
        "        def configure_optimizers(self):\n",
        "            optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
        "            #optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "            return optimizer\n",
        "\n",
        "    # シードを固定して再現性を確保\n",
        "    pl.seed_everything(0)\n",
        "\n",
        "    ###\n",
        "    # 学習を行う Trainer\n",
        "    net = Network()\n",
        "    #trainer = pl.Trainer(gpus=1, max_epochs=30, deterministic=True)\n",
        "    #trainer = pl.Trainer(devices=1, max_epochs=30, deterministic=True)\n",
        "    #trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=30, deterministic=True)\n",
        "    trainer = pl.Trainer(accelerator='auto', devices=AVAIL_GPUS, max_epochs=EPOCK, deterministic=True,\n",
        "        callbacks=[\n",
        "            # early stopping\n",
        "            #pl.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=15, verbose=VERBOSE),\n",
        "            pl.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=10, verbose=VERBOSE),\n",
        "            # checkpoint,\n",
        "            pl.callbacks.ModelCheckpoint(save_top_k=1, monitor=\"val_acc\", mode=\"max\",verbose=VERBOSE),\n",
        "            #pl.callbacks.LearningRateMonitor(logging_interval='epoch',verbose=verbose),\n",
        "            pl.callbacks.TQDMProgressBar(refresh_rate=0 if not VERBOSE else 10),\n",
        "        ],\n",
        "        enable_checkpointing=True,\n",
        "        logger= False\n",
        "    )\n",
        "    return net, train_loader, val_loader, test_loader, trainer"
      ],
      "metadata": {
        "id": "Rb_dF9_5Uh3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 分類の実行"
      ],
      "metadata": {
        "id": "4lDuWbqKU2mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# DNNネット構造を作成\n",
        "_total_size, in_dim, out_dim, train, val, test = obtain_corpus_train_val_test(0)\n",
        "net, _train_loader, _val_loader, _test_loader, trainer = generate_DNN_model(in_dim, out_dim, train, val, test)\n",
        "\n",
        "# 入力文埋め込みの生成\n",
        "_num, sentences, emotion_data = read_excel_file(INPUT_FILE)\n",
        "sentence_embeddings = cyclone_simcse_model.encode(sentences)\n",
        "tensor_embeddings = torch.tensor(sentence_embeddings, dtype=torch.float32)\n",
        "\n",
        "# 格納済みモデルを読み込み\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1s = []\n",
        "for e, category in enumerate(EMOTION_CATEGORIES):\n",
        "    print(category)\n",
        "    category_vector = emotion_data[e]\n",
        "    print(category + \"がポジティブの教師文数：\", np.count_nonzero(category_vector))\n",
        "    net.load_state_dict(torch.load(MODEL_SAVE_PATH + category + \".pt\"))\n",
        "    #\n",
        "    net.eval()\n",
        "    #\n",
        "    out = net(tensor_embeddings)\n",
        "    out_argmax = torch.argmax(out, dim=1)\n",
        "    #print(out_argmax.shape)\n",
        "    #print(out_argmax)\n",
        "    print(category + \"がポジティブの予測文数：\", torch.count_nonzero(out_argmax).numpy())\n",
        "    #print(category_vector.shape, out_argmax.numpy().shape)\n",
        "    #print(category_vector[:5], out_argmax.numpy()[:5])\n",
        "    precision = precision_score(category_vector, out_argmax.numpy())\n",
        "    recall = recall_score(category_vector, out_argmax.numpy())\n",
        "    f1 = f1_score(category_vector, out_argmax.numpy())\n",
        "    print(\"Precision:\",precision,\"Recall:\",recall, \"F1:\",f1)\n",
        "    precisions.append(precision)\n",
        "    recalls.append(recall)\n",
        "    f1s.append(f1)\n",
        "    print()\n",
        "\n",
        "print(\"Average precision:\", np.mean(precisions))\n",
        "print(\"Average recall:\", np.mean(recalls))\n",
        "print(\"Average F1:\", np.mean(f1s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVddKsmSU3A_",
        "outputId": "989c2719-2cef-4282-8660-828a12fea1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpusの読込\n",
            "文埋め込みの生成開始\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 0\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "各感情ベクトルの生成\n",
            "joy\n",
            "joyがポジティブの教師文数： 3127\n",
            "joyがポジティブの予測文数： 3048\n",
            "Precision: 0.9530839895013123 Recall: 0.9290054365206268 F1: 0.9408906882591094\n",
            "\n",
            "anger\n",
            "angerがポジティブの教師文数： 1186\n",
            "angerがポジティブの予測文数： 910\n",
            "Precision: 0.6505494505494506 Recall: 0.4991568296795953 F1: 0.564885496183206\n",
            "\n",
            "sadness\n",
            "sadnessがポジティブの教師文数： 477\n",
            "sadnessがポジティブの予測文数： 231\n",
            "Precision: 0.8095238095238095 Recall: 0.3920335429769392 F1: 0.5282485875706214\n",
            "\n",
            "fear\n",
            "fearがポジティブの教師文数： 192\n",
            "fearがポジティブの予測文数： 106\n",
            "Precision: 0.8301886792452831 Recall: 0.4583333333333333 F1: 0.5906040268456376\n",
            "\n",
            "shame\n",
            "shameがポジティブの教師文数： 40\n",
            "shameがポジティブの予測文数： 0\n",
            "Precision: 0.0 Recall: 0.0 F1: 0.0\n",
            "\n",
            "like\n",
            "likeがポジティブの教師文数： 1737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "likeがポジティブの予測文数： 1849\n",
            "Precision: 0.8561384532179557 Recall: 0.9113413932066782 F1: 0.8828778583379812\n",
            "\n",
            "dislike\n",
            "dislikeがポジティブの教師文数： 2972\n",
            "dislikeがポジティブの予測文数： 2964\n",
            "Precision: 0.868421052631579 Recall: 0.8660834454912517 F1: 0.8672506738544474\n",
            "\n",
            "excitement\n",
            "excitementがポジティブの教師文数： 113\n",
            "excitementがポジティブの予測文数： 29\n",
            "Precision: 0.9655172413793104 Recall: 0.24778761061946902 F1: 0.39436619718309857\n",
            "\n",
            "peacefulness\n",
            "peacefulnessがポジティブの教師文数： 207\n",
            "peacefulnessがポジティブの予測文数： 107\n",
            "Precision: 0.8504672897196262 Recall: 0.4396135265700483 F1: 0.5796178343949046\n",
            "\n",
            "surprise\n",
            "surpriseがポジティブの教師文数： 263\n",
            "surpriseがポジティブの予測文数： 108\n",
            "Precision: 0.8055555555555556 Recall: 0.33079847908745247 F1: 0.46900269541778977\n",
            "\n",
            "request\n",
            "requestがポジティブの教師文数： 556\n",
            "requestがポジティブの予測文数： 488\n",
            "Precision: 0.944672131147541 Recall: 0.829136690647482 F1: 0.8831417624521072\n",
            "\n",
            "Average precision: 0.775828877497402\n",
            "Average recall: 0.5366627534666251\n",
            "Average F1: 0.609171438227173\n"
          ]
        }
      ]
    }
  ]
}