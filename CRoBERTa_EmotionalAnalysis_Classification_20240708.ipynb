{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zmy6208/zmy6208/blob/main/CRoBERTa_EmotionalAnalysis_Classification_20240708.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjYKosTKQ3Rp"
      },
      "source": [
        "# 実行環境の設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WbLTvoFP0KF"
      },
      "outputs": [],
      "source": [
        "# GPUおよびPython環境のチェック\n",
        "#import locale\n",
        "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!nvidia-smi\n",
        "!python --version\n",
        "\n",
        "# Google Drive のマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8DgTnl_RPhC"
      },
      "source": [
        "# パッケージインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4dks9DARNFp"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece transformers pytorch-lightning logzero\n",
        "import pytorch_lightning as pl\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "\n",
        "#\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import openpyxl as op\n",
        "import shutil\n",
        "import json\n",
        "import sys\n",
        "import os, torch, datetime\n",
        "import glob\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install logzero\n",
        "from logzero import logger\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiCnu6CeLQyo"
      },
      "source": [
        "# ファイルパス"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hscM-GaPLbO3"
      },
      "outputs": [],
      "source": [
        "# パス分離記号\n",
        "PATH_SEPARATOR = \"/\"\n",
        "\n",
        "###\n",
        "# プロジェクトルート\n",
        "PROJECT_ROOT = \"/content/gdrive/MyDrive/CRoBERTa/\"\n",
        "\n",
        "# CRoBERTa pretrained_model path\n",
        "PRETRAINED_MODEL = PROJECT_ROOT + \"chinese-roberta-wwm-ext\"\n",
        "\n",
        "# 訓練データを格納するディレクトリ\n",
        "TRAINING_DATA_DIR = PROJECT_ROOT + \"corpus/\"\n",
        "TRAINING_DATA = TRAINING_DATA_DIR + \"Corpus-Review+Corpus-Novel.xlsx\"\n",
        "\n",
        "# Fine-tuning済みモデルの保存先\n",
        "MODEL_SAVE_DIR = PROJECT_ROOT + \"classification/models/\"\n",
        "\n",
        "# 11感情分類CRoBERTaモデル\n",
        "SELECTED_MODEL_DIR = PROJECT_ROOT + \"classification/selected/\"    # 11感情分類モデルの保存先\n",
        "\n",
        "# 分類ターゲットデータ名\n",
        "TARGET_DATA_NAME = \"target.xlsx\"                                # 分類対象ファイル名に変更する！\n",
        "\n",
        "# 分類ターゲットデータ保存先\n",
        "CLASSIFICATION_INPUT_DIR = PROJECT_ROOT + \"data/input/\"\n",
        "CLASSIFICATION_INPUT_FILENAME = CLASSIFICATION_INPUT_DIR + TARGET_DATA_NAME\n",
        "\n",
        "# 分類結果保存先\n",
        "CLASSIFICATION_RESULT_DIR = PROJECT_ROOT + \"data/result/\" + TARGET_DATA_NAME[:-5]\n",
        "CLASSIFICATION_RESULT_FILENAME = CLASSIFICATION_RESULT_DIR + PATH_SEPARATOR + \"分類結果.xlsx\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YixiK2HBLk0u"
      },
      "source": [
        "# 学習モデルおよびパラメータ設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPAa1wGCxtz7"
      },
      "outputs": [],
      "source": [
        "# 11感情カテゴリのリスト\n",
        "TARGET_EMOTIONS = [\"喜\", \"怒\", \"哀\", \"怖\", \"恥\", \"好\", \"厭\", \"昂\", \"安\", \"驚\", \"望\"]\n",
        "\n",
        "# 事前学習モデルの名前(Huggingface)\n",
        "MODEL = AutoModel.from_pretrained(PRETRAINED_MODEL)\n",
        "# Tokenizer\n",
        "TOKENIZER = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
        "TOKENIZER.do_lower_case = True\n",
        "\n",
        "# 計算資源 (GPUが使用できる場合はCUDA, できない場合はCPUが選択される)\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# 学習パラメータ\n",
        "#EPOCHS = 3\n",
        "#TRAIN_BATCH_SIZE = 32\n",
        "#VAL_BATCH_SIZE = 64\n",
        "#TEST_BATCH_SIZE = 64\n",
        "PREDICT_BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB5JYjDmSccZ"
      },
      "source": [
        "# ファイル入出力ユーティリティ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNTVKeAWSjOz"
      },
      "outputs": [],
      "source": [
        "### jsonファイル入出力\n",
        "# jsonファイルの読み込み：必要な列数に書き換える\n",
        "def read_json_file(JSON_DATAFILE):\n",
        "    pd_data = pd.read_json(JSON_DATAFILE)\n",
        "    ndaray_data = pd_data.to_numpy(copy=True).transpose()\n",
        "    list_first_column = list(ndaray_data[0])\n",
        "    list_second_column = list(ndaray_data[1])\n",
        "    #print(list_first_column)\n",
        "    #print(list_second_column)\n",
        "    return list_first_column, list_second_column\n",
        "\n",
        "# 2次元list_dataリストのjsonファイルへの書き出し\n",
        "def write_json_file(JSON_DATAFILE, list_data):\n",
        "    with open(JSON_DATAFILE, \"w\") as f:\n",
        "        json.dump(list_data, f)\n",
        "\n",
        "### excelファイル入出力\n",
        "# excelデータの読み込み\n",
        "def read_excel_file(EXCEL_DATAFILE):\n",
        "    input_book = pd.ExcelFile(EXCEL_DATAFILE)\n",
        "    input_sheet_name = input_book.sheet_names\n",
        "    input_sheet_df = input_book.parse(input_sheet_name[0])\n",
        "    transposed_df = input_sheet_df.transpose()\n",
        "    data = transposed_df.values.tolist()\n",
        "    #print(\"Number of columns:\", len(data))\n",
        "    if len(data) == 13:                              # Chinese corpus for emotional analysis\n",
        "        sentences = data[1]\n",
        "        emotion_data = np.array(data[2:13])\n",
        "    else:\n",
        "        print(\"Excel : not supported format\")\n",
        "        sys.exit()\n",
        "    return len(data), sentences, emotion_data\n",
        "\n",
        "# excelファイルへの書き出し\n",
        "def write_excel_file(EXCEL_DATAFILE, pd_data):\n",
        "    pd_data.to_excel(EXCEL_DATAFILE, index=False, header=True)\n",
        "\n",
        "## textファイルの入出力\n",
        "# textデータの読み込み\n",
        "def read_text_file(TEXT_DATAFILE):\n",
        "    sentences = []\n",
        "    with open(TEXT_DATAFILE,mode='r', encoding='utf-8') as f:\n",
        "        for s in f:\n",
        "            try:\n",
        "                sentences.append(s.rstrip())\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return sentences\n",
        "\n",
        "# textファイルへの書き出し\n",
        "def write_text_file(TEXT_DATAFILE, sentences):\n",
        "    with open(TEXT_DATAFILE, mode='w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZuTGVSxETB"
      },
      "source": [
        "# 11感情分類実験用モデルを選択：！変更しないときは実行しない！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_lYI7QQxD1r"
      },
      "outputs": [],
      "source": [
        "### モデルを変更しないときは実行しない\n",
        "#\n",
        "def select_finetuned_model():\n",
        "    for e in TARGET_EMOTIONS:\n",
        "        files = []\n",
        "        for fname in os.listdir(MODEL_SAVE_DIR + e):\n",
        "            if fname.endswith(\".ckpt\"):\n",
        "                #print(fname)\n",
        "                files.append(fname)\n",
        "        if len(files) == 0:\n",
        "            print(e + \"モデルは\", MODEL_SAVE_DIR + e + PATH_SEPARATOR, \"にありません。\")\n",
        "            continue\n",
        "        elif len(files) == 1:\n",
        "            os.makedirs(SELECTED_MODEL_DIR + e, exist_ok=True)\n",
        "            shutil.copy(MODEL_SAVE_DIR + e + PATH_SEPARATOR + files[0], SELECTED_MODEL_DIR + e + PATH_SEPARATOR)\n",
        "            print(e + \"モデルを\" + SELECTED_MODEL_DIR + e + PATH_SEPARATOR + files[0] + \"にコピーしました。\")\n",
        "        elif len(files) > 1:\n",
        "            sorted_files = sorted(files)\n",
        "            print('モデルをリストにします。(番号： モデル名)')\n",
        "            nf = 0\n",
        "            for fname in sorted_files:\n",
        "                nf += 1\n",
        "                print(str(nf) + \": \" +  fname)\n",
        "            print('q: 終了')\n",
        "            print()\n",
        "            while True:\n",
        "                nstr = input('ファイル番号を入力してください。(コピーしない：q または Q) >> ')\n",
        "                if nstr == 'q' or nstr == 'Q':\n",
        "                    print(e + \"モデルはコピーしません。\")\n",
        "                    break\n",
        "                try:\n",
        "                    nfile = int(nstr) - 1\n",
        "                except Exception:\n",
        "                    print('1以上' + str(nf) + '以下のファイル番号（半角数字）を入力してください。')\n",
        "                    print()\n",
        "                    continue\n",
        "                if nfile < 0 and nf <= nfile:\n",
        "                    print('1以上' + str(nf) + '以下のファイル番号（半角数字）を入力してください。')\n",
        "                    print()\n",
        "                    continue\n",
        "            os.makedirs(SELECTED_MODEL_DIR + e, exist_ok=True)\n",
        "            shutil.copy(MODEL_SAVE_DIR + e + PATH_SEPARATOR + files[nfile], SELECTED_MODEL_DIR + e + PATH_SEPARATOR)\n",
        "            print(e + \"モデルを\" + SELECTED_MODEL_DIR + e + PATH_SEPARATOR + files[nfile] + \"にコピーしました。\")\n",
        "\n",
        "select_finetuned_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6s0TPn7WSC8"
      },
      "source": [
        "# 11感情分類データの前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5PbhDzpWSix"
      },
      "outputs": [],
      "source": [
        "## targetデータの前処理\n",
        "class ExcelDataset():\n",
        "    def __init__(self, tokenizer, add_cls=False) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.add_cls = add_cls\n",
        "\n",
        "    # 文の前に\"[CLS]\"を追加する関数 (rinna RoBERTa用)\n",
        "    # (※ rinna RoBERTaのT5Tokenizerでは[CLS]トークン自動で追加しないため、自分で分析文の前に追加する)\n",
        "    # 詳細：https://huggingface.co/rinna/japanese-roberta-base\n",
        "    def _add_cls(self, sentence: str) -> str:\n",
        "        return \"[CLS]\" + sentence\n",
        "\n",
        "    # Finetuningデータ全体から求めたmax_length（最大トークン数）を読み込む関数\n",
        "    def _get_max_length(self) -> int:\n",
        "        max_length = read_text_file(MODEL_SAVE_DIR + \"max_length\")\n",
        "        max_length = int(max_length[0])\n",
        "        #print(\"max_length:\", max_length)\n",
        "        return max_length\n",
        "\n",
        "    # targetの文データを ID 化し、RoBERTaに入力できる形に変換する\n",
        "    def build(self, file_dir: str) -> tuple:\n",
        "        target = []\n",
        "\n",
        "        target_df = pd.read_excel(os.path.join(file_dir, CLASSIFICATION_INPUT_FILENAME))\n",
        "        max_length = self._get_max_length()\n",
        "\n",
        "        logger.info(\"Loading target data\")\n",
        "        for _index, data in target_df.iterrows():\n",
        "            #print(type(data),type(data[0]),data[0])\n",
        "            sentence = self._add_cls(data[\"sentence\"]) if self.add_cls else data[\"sentence\"]\n",
        "            encoding = self.tokenizer.encode_plus(sentence, max_length=max_length, padding=\"max_length\")\n",
        "            encoding[\"labels\"] = 0\n",
        "            encoding = {k: torch.tensor(v) for k, v in encoding.items()}\n",
        "            target.append(encoding)\n",
        "        logger.info(\"Loading completed\")\n",
        "        return target\n",
        "\n",
        "#dataset = ExcelDataset(tokenizer=TOKENIZER, add_cls=True)          # rinna RoBERTa用\n",
        "dataset = ExcelDataset(tokenizer=TOKENIZER, add_cls=False)          # CRoBERTa用\n",
        "\n",
        "\n",
        "# targetデータの読み込み\n",
        "target = dataset.build(\n",
        "    file_dir=TRAINING_DATA_DIR\n",
        ")\n",
        "\n",
        "# ミニバッチ化\n",
        "dataloader_target = DataLoader(target, batch_size=PREDICT_BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_Xm1OFxS616"
      },
      "source": [
        "# RoBERTaモデルの定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuDNoWGMup6q"
      },
      "outputs": [],
      "source": [
        "# モデルのラッパークラス\n",
        "class RobertaForSequenceClassification_pl(pl.LightningModule):\n",
        "    def __init__(self, model_name, num_labels, lr):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # 事前学習済みRoBERTaの読み込み\n",
        "        self.bert_sc = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        output = self.bert_sc(**batch)\n",
        "        loss = output.loss\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        output = self.bert_sc(**batch)\n",
        "        val_loss = output.loss\n",
        "        self.log(\"val_loss\", val_loss)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        global result_df\n",
        "        labels = batch.pop(\"labels\")\n",
        "        output = self.bert_sc(**batch)\n",
        "        labels_predicted = output.logits.argmax(dim=-1)\n",
        "        num_correct = (labels_predicted==labels).sum().item()\n",
        "        accuracy = num_correct / labels.size(0)\n",
        "        self.log(\"accuracy\", accuracy)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11感情分類の実行"
      ],
      "metadata": {
        "id": "FFWyE9d6EWAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdzu7V6bRhaB"
      },
      "outputs": [],
      "source": [
        "def CRoBERTa_clssification(emotion):\n",
        "    # 訓練済みモデルの読み込み\n",
        "    #model_pl = RobertaForSequenceClassification_pl.load_from_checkpoint(best_model_path)\n",
        "    # model_pl = RobertaForSequenceClassification_pl.load_from_checkpoint(f\"{save_model_dir}/{***.ckpt}\")\n",
        "    model_pl = None\n",
        "    for fname in os.listdir(SELECTED_MODEL_DIR + emotion):\n",
        "        if fname.endswith(\".ckpt\"):\n",
        "            print(fname)\n",
        "            model_pl = RobertaForSequenceClassification_pl.load_from_checkpoint(SELECTED_MODEL_DIR + emotion + PATH_SEPARATOR + fname)\n",
        "\n",
        "    # ラッパーからRoBERTaモデルを取り出す\n",
        "    model = model_pl.bert_sc.to(DEVICE)\n",
        "\n",
        "    # 教師ラベル と 予測ラベル のリスト\n",
        "    pred_labels = []\n",
        "\n",
        "    # テストデータのミニバッチを1つずつ取り出し、予測値をリストに格納する\n",
        "    for batch in tqdm(dataloader_target):\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            output = model(**batch)\n",
        "        score = output.logits\n",
        "        labels_predicted = score.argmax(dim=-1)\n",
        "        pred_labels.extend(labels_predicted.cpu().numpy())\n",
        "\n",
        "    del batch, output, score, labels_predicted\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return pred_labels\n",
        "\n",
        "###\n",
        "# 文と予測値のDataFrameを作成\n",
        "\n",
        "# target.xlsx の文のDataFrameを構成\n",
        "sentences = pd.read_excel(os.path.join(CLASSIFICATION_INPUT_DIR, CLASSIFICATION_INPUT_FILENAME))[\"sentence\"].to_list()\n",
        "sentence_df = pd.DataFrame(sentences, columns=[\"sentence\"])\n",
        "#print(sentence_df)\n",
        "\n",
        "# 11感情分類予測値のDataFrameｂのリストを構成\n",
        "pred_labels_df_e_list = []\n",
        "for e in TARGET_EMOTIONS:\n",
        "    pred_labels = CRoBERTa_clssification(e)\n",
        "    pred_labels_df_e = pd.DataFrame(pred_labels, columns=[e])\n",
        "    pred_labels_df_e_list.append(pred_labels_df_e)\n",
        "\n",
        "# 文と分類結果を連結したDataFrameを構成\n",
        "pred_labels_df = pd.concat([sentence_df] + pred_labels_df_e_list, axis=1)\n",
        "#print(pred_labels_df)\n",
        "\n",
        "# 分析結果の保存先\n",
        "result_dir = CLASSIFICATION_RESULT_DIR\n",
        "# 保存先がない場合は作成\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "# 分析結果を保存\n",
        "write_excel_file(CLASSIFICATION_RESULT_FILENAME, pred_labels_df)\n",
        "print(\"11感情分類結果を\" + CLASSIFICATION_RESULT_FILENAME + \"に出力しました。\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}